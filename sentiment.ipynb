{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Python\n",
    "This is an example of training a sentiment classifier with Python. \n",
    "\n",
    "The aims of this hands-on experiment are to present:\n",
    "- the basics of data analysis\n",
    "- how to pre-process a dataset and why it is important\n",
    "- the (very) basics of supervised machine learning\n",
    "- analysis of a classifier's results\n",
    "\n",
    "We will use the [Women's E-commerce Clothing Review](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) dataset with their rating scores.\n",
    "Rating scores goes from 1 to 5, where 1 is the worst rating and 5 the best.\n",
    "\n",
    "## You're using a notebook!\n",
    "This is a 'notebook' and it allows us to work with a programming language called python. The notebook has cells. Some cells (like this one) are text. Others (like the one below) are code. It can be a little confusing! You can run code by clicking on the cell, then clicking the 'play' button on the left hand side (or pressing Ctrl-Enter). Try it for the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = \"chocolate\"\n",
    "print(\"My favourite food is \" + food + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "Let's get started! We need to get things set up, you can just run the next cell & move on, as this just gets things installed that we need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by the way, in python text after a hash (#) is a comment! Like this!\n",
    "\n",
    "#loading some libraries\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download and read the dataset\n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/lionfish0/discover_stem/master/Womens%20Clothing%20E-Commerce%20Reviews.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Try to print only the 'Rating' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##exercise 1 area\n",
    "#Here we print out their ages by adding ['Age'] to the dataset variable:\n",
    "dataset['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#playing with graphs (here are just some basic configurations)\n",
    "plot_size = plt.rcParams[\"figure.figsize\"] \n",
    "plot_size[0] = 10\n",
    "plot_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = plot_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting the distribution of ratings\n",
    "dataset['Rating'].value_counts().plot(kind='pie', autopct='%1.0f%%')\n",
    "\n",
    "#if you want a bar graph, you could uncomment (remove the #) from these three lines instead.\n",
    "#dataset['Rating'].value_counts().plot(kind='bar')\n",
    "#plt.xlabel('Rating')\n",
    "#plt.ylabel('Number of reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Try making the pie chart show the information in a different column. For example 'Department Name' or 'Class name', by modifying the code above. (tip: try replacing 'Rating' with 'Department Name')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##exercise 2 area\n",
    "#dataset['Rating'].value_counts().plot(kind='pie', autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why this graph can be misleading? \n",
    "clothes_sentiment = dataset.groupby(['Department Name', 'Rating'])['Rating'].count().unstack()\n",
    "clothes_sentiment.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Analysing according to the counts of ratings\n",
    "clothes_sentiment_count = dataset.groupby(['Department Name', 'Rating'])['Rating'].count().unstack()\n",
    "print(\"** Number of each rating per department **\")\n",
    "print(clothes_sentiment_count)\n",
    "#sum per deparment\n",
    "print(\"** Sum of ratings per deparment **\")\n",
    "dept_sum = clothes_sentiment_count.sum(axis=1)\n",
    "print(dept_sum)\n",
    "#percentage\n",
    "clothes_sentiment_perc = (clothes_sentiment_count.transpose()/dept_sum).transpose()\n",
    "print(\"** Percentage of each rating per department **\")\n",
    "print(clothes_sentiment_perc)\n",
    "clothes_sentiment_perc.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysing the text: any ideas of potential problems? \n",
    "dataset['Review Text'][2] #this is review number 2, try changing the '2', to read other reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the review column into an array of reviews\n",
    "features = np.array(dataset['Review Text'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the vector with the labels\n",
    "#we are using the 'Rating' column as our labels, so we have 5 classes \n",
    "labels = [int(l) for l in dataset['Rating']]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into training, validation and test sets\n",
    "#training = data used to train the classifiers\n",
    "#validation = data used to tune the classifiers' parameters (it will make more sense later)\n",
    "#test = data used to test the classifiers\n",
    "raw_train, raw_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)\n",
    "raw_train, raw_val, y_train, y_val = train_test_split(raw_train, y_train, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing the reviews\n",
    "#normalise words, remove punctuation, remove extra spaces, etc\n",
    "def pre_proc(features):\n",
    "    processed_features = []\n",
    "    for sentence in range(0, len(features)):\n",
    "        # Remove all tags (like <br />)\n",
    "        processed_feature = re.sub(r'<.*?>', ' ', str(features[sentence]))\n",
    "\n",
    "        #Remove all special characters\n",
    "        processed_feature = re.sub(r'[^a-zA-Z0-9]', ' ', processed_feature)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "        \n",
    "        # Removing everything that has numbers \n",
    "        #processed_feature = re.sub(r'\\w*\\d\\w*', '', processed_feature)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        processed_feature = processed_feature.lower()\n",
    "\n",
    "        processed_features.append(processed_feature)\n",
    "    return processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply pre_proc() function to all data splits\n",
    "proc_train = pre_proc(raw_train)\n",
    "proc_val = pre_proc(raw_val)\n",
    "proc_test = pre_proc(raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Change the 'i' variable to see different examples of the original and pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(\"ORIGINAL: %s\" % raw_train[i])\n",
    "print()\n",
    "print(\"PRE-PROCESSED: %s\" % proc_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "We will use bag-of-words as features for training our classifiers. In a bag-of-words approach, an algorithm counts the number of times a word appear in a document. Each word in the entire collection of documents (corpus) became a feature in the feature vector, which results in a sparse vector.\n",
    "\n",
    "Instead of count the \"number of times\" a word appear in a document, we can also use a binary approach (whether or not a word a appear in a document). Any other ideas? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_features(binary=True, max_df=1.0, min_df=0.0, ngram_range=(1,1), sw=False):\n",
    "#By default, we are using a binary bag-of-words approach: \n",
    "#if a word appears in a document it will receive 1 (0 otherwise)\n",
    "\n",
    "    stop_words=[]\n",
    "    if sw:\n",
    "        f = urlopen(\"https://raw.githubusercontent.com/lionfish0/discover_stem/master/stopwords.txt\").read()\n",
    "        stop_words = list(np.array(f.split(), dtype=str))\n",
    "\n",
    "    cv = CountVectorizer(binary=True, max_df=max_df, min_df=min_df, ngram_range=ngram_range, stop_words=stop_words)\n",
    "    cv.fit(proc_train)\n",
    "\n",
    "    #check the features outputted below\n",
    "    #each possible word in our pre-processed vector became a feature\n",
    "    #can we do better? \n",
    "    print(\"** Vocabulary size: %d\" % len(cv.get_feature_names()))\n",
    "    print(\"** Words:\")\n",
    "    print(cv.get_feature_names())\n",
    "\n",
    "    #apply the model to all data splits\n",
    "    #can you think of any problems? \n",
    "    X_train = cv.transform(proc_train)\n",
    "    X_val = cv.transform(proc_val)\n",
    "    X_test = cv.transform(proc_test)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the above function to extract features\n",
    "X_train, X_val, X_test = extract_features(binary=True, max_df=1.0, min_df=0.0, ngram_range=(1,1), sw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a function to print a nice confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "class_names = ['1', '2', '3', '4', '5']\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: majority class classifier\n",
    "Predicts all instances as the majority class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the majority class classifier\n",
    "dc = DummyClassifier(strategy=\"most_frequent\")\n",
    "dc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation function\n",
    "def evaluate_classifier(cls, X_test, y_test):\n",
    "    \n",
    "    preds = cls.predict(X_test)\n",
    "    print(classification_report(y_test, preds))\n",
    "    ax = plot_confusion_matrix(y_test, preds, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating it\n",
    "evaluate_classifier(dc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First experiment: K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_knn(X_train, y_train, n_neighbors=3):  \n",
    "    #training the classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = train_knn(n_neighbors=3)\n",
    "evaluate_classifier(knn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Vary the value of 'n_neighbors' and see if you can improve the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = train_knn(X_train, y_train, n_neighbors=10)\n",
    "evaluate_classifier(knn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Vary the value of 'i' to see more examples of the data and their predicted and true ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print some samples of the data and compare predicted and true values\n",
    "i = 1\n",
    "print(raw_test[i])\n",
    "print()\n",
    "print(\"** Predicted value by KNN: %d\" % knn_preds[i])\n",
    "print(\"** True value: %d\" % y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second experiment: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_C(X_train, y_train, X_val, y_val, C=[0.01, 0.025, 0.05, 0.25, 0.5, 1.0]):\n",
    "    #optimise the parameter C using the validation data\n",
    "    best_acc = 0.\n",
    "    best_c = 0.\n",
    "    accuracies = []\n",
    "    for c in C:\n",
    "    \n",
    "        lr = LogisticRegression(C=c, multi_class='auto', solver='liblinear')\n",
    "        lr.fit(X_train, y_train)\n",
    "        cur_acc = accuracy_score(y_val, lr.predict(X_val))\n",
    "        print (\"Accuracy for C=%s: %s\" % (c, cur_acc))\n",
    "        accuracies.append(cur_acc)\n",
    "        if cur_acc > best_acc:\n",
    "            best_c = c\n",
    "            best_acc = cur_acc\n",
    "\n",
    "    print (\"*** Best accuracy = %f, best C = %f\" % (best_acc, best_c))\n",
    "    plt.plot(np.array(C).astype('str'), accuracies, 'ro')\n",
    "    return best_c\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = optimise_C(X_train, y_train, X_val, y_val, C=[0.01, 0.025, 0.05, 0.25, 0.5, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Vary the values of the C list (positive float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = optimise_C(X_train, y_train, X_val, y_val, C=[0.01, 0.025, 0.05, 0.25, 0.5, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr(X_train, y_train, best_c=1.0):\n",
    "    #training the model with the best C\n",
    "    lr = LogisticRegression(C=best_c, multi_class='auto', solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = train_lr(X_train, y_train, best_c=best_c)\n",
    "evaluate_classifier(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Try to change the features and re-train the KNN and Logistic Regression classifiers. \n",
    "- What happens if you filter out words that are too frequent or less frequent? \n",
    "- What happens if we use bigrams our trigrams? \n",
    "- What happens if we use a stop-word list?\n",
    "- What happens if we use counts of words instead of the binary approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 4 area\n",
    "\n",
    "#change the parameters below to solve the exercise\n",
    "#binary: whether or not we use the binary approach\n",
    "#max_df: maximum frequency\n",
    "#min_df: minimum frequency\n",
    "#ngram_range: n-grams considered \n",
    "#sw: whehter or not we should use a stopwords list\n",
    "X_train, X_val, X_test = extract_features(binary=False, max_df=1.0, min_df=0.0, ngram_range=(1,1), sw=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##KNN - you can also change n_neighbors values\n",
    "knn = train_knn(X_train, y_train, n_neighbors=3)\n",
    "evaluate_classifier(knn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LR - you can also change C values\n",
    "best_c = optimise_C(X_train, y_train, X_val, y_val, C=[0.01, 0.025, 0.05, 0.25, 0.5, 1.0])\n",
    "lr = train_lr(X_train, y_train, best_c=best_c)\n",
    "evaluate_classifier(lr, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
